---
aliases:
  - /binding site/pocket/cavity/pocket comparison/structure-based drug design/2023/05/10/binding-site-comparison-benchmark-II
author: Peter Schmidtke
badges: true
branch: master
categories:
  - binding site
  - pocket
  - cavity
  - pocket comparison
  - structure-based drug design
date: '2023-05-10'
bibliography: refs.bib
description: Second post on establishing a benchmark for binding site comparison methods on similar proteins
image: toc.png
output-file: 2023-05-10-binding-site-comparison-benchmark-ii.html
title: Binding Site Comparison Benchmarks - II - Binding sites on the similar proteins
from: markdown+emoji
toc: true
jupyter: python3
execute:
  freeze: auto
---

# Introduction

[The previous post](../2023-04-03-binding-site-comparison-benchmark-I/2023-04-03-binding-site-comparison-benchmark-I.qmd) of this series established a first protocol on how to identify & cluster the ATP binding site of the N-terminal domain of human HSP90 alpha. It can be generalized to other proteins as well but to keep it simple I'll continue this series on a single protein for now and generalize at a later stage.

In this second post we'll gradually extend the dataset with ATP binding sites of proteins that are very similar to the human HSP90 alpha. 

#### What is similar?
One can adopt several ways to assess the similarity between proteins. Here I'll use a commonly understood and accepted sequence local similarity on the N-terminal domain. I'll further assess the binding site residue conservation in detail. So at this stage of the dataset creation I'll focus on things I'd expect to be found as very very similar to the human HSP90 alpha N-terminal domain when doing a pocket comparison. 
The Rognan lab discussed this topic in more than one paper. Feel free to check out @Kellenberger2008 and @Eguida2022 for more details.

#### Things to keep in mind
Again I'll track all things that I left aside or important approximations I made. They'll be tracked as side-note and depending on the outcome of the whole exercise I'll come back to them at a later stage.

::: column-margin
Things to keep in mind

- listing goes here
:::

# Methods & Results

## Identify similar sequences

In order to identify similar sequences one usually would do just a simple blast. Unfortunately, things are a bit trickier. First of all, we are currently looking only at the N-terminal domain of HSP90 (around 230 amino acids). The full human HSP90 alpha protein has 732 amino acids. So, shall we search for globally similar sequences? or smaller regions?

The N-terminal part is considered to be a domain, so structured on itself. There are numerous structure resolution papers that show that, @Sreeramulu2009 is just one of them, as an example here, where only the N-terminal domain of HSP90 is amplified, produced & resolved. 

We could even go further and use only the range of amino acids that are involved in the ATP binding site. In the case of a small domain like this one here that might not be required, but for large domains it might be of interest to focus on the binding site only. In this example this would mean start at the glutamate 47 and go up to the valine 186 (+ maybe a few flanking residues)

For the sake of simplicity I'll take the whole N-terminal domain (amino acids 9-236), corresponding to the [first region of HS90A_HUMAN in the UniProt database](https://www.uniprot.org/uniprotkb/P07900/entry#family_and_domains).

### The blast mess

Now let's try to set up a first blast search. We want to find PDB structures containing a resolved portion of the domain of interest (or similar). There are unfortunately thousands of services allowing to do that, and as usual they don't give the same results. One important factor is to search against an up to date version of the RCSB PDB. The only service I found that clearly states what release of the PDB is being used is [NCBI blastp page](https://blast.ncbi.nlm.nih.gov/Blast.cgi?ALIGNMENTS=50&ALIGNMENT_VIEW=Pairwise&AUTO_FORMAT=Semiauto&CLIENT=web&DATABASE=pdb&DESCRIPTIONS=100&ENTREZ_QUERY=(none)&EXPECT=20000&FORMAT_BLOCK_ON_RESPAGE=None&FORMAT_ENTREZ_QUERY=(none)&FORMAT_OBJECT=Alignment&FORMAT_TYPE=HTML&GAPCOSTS=9+1&I_THRESH=0.005&LAYOUT=TwoWindows&MATRIX_NAME=PAM30&NCBI_GI=on&PAGE=Proteins&PROGRAM=blastp&QUERY=gapn&SERVICE=plain&SET_DEFAULTS.x=14&SET_DEFAULTS.y=5&SHOW_LINKOUT=on&SHOW_OVERVIEW=on&WORD_SIZE=2&END_OF_HTTPGET=Yes).

I don't want to reinvent the wheel or install blast locally etc, so here I'll use the biotite (@Kunzmann2018) integration of the NCBI blast service. 

```{python}
#| eval: false
#| code-fold: false
#| 
import biotite.sequence.io.fasta as fasta
import biotite.application.blast as blast
from biotite.sequence import ProteinSequence
import urllib
import io 
import pickle

uniprotcode="P07900"
start=9
end=236
url="https://rest.uniprot.org/uniprotkb/"+uniprotcode+".fasta"

with urllib.request.urlopen(url) as file_handle:
  file_object = io.TextIOWrapper(file_handle)
  fasta_file = fasta.FastaFile.read(file_object)
  for header, seq_str in fasta_file.items():
    seq = ProteinSequence(seq_str)
    selected_sequence = str(seq[start:end])

app = blast.BlastWebApp("blastp", selected_sequence, database="pdb",obey_rules=True)
app.set_max_results(5000)
app.set_max_expect_value(1e-30)
app.start()
app.join(timeout=1000)
alignments = app.get_alignments()
xml=app.get_xml_response()

# open a file, where you ant to store the data
file = open('data/blasthits.pkl', 'wb')

# dump information to that file
pickle.dump(xml, file)

# close the file
file.close()

```

Here we get all blast hits with an e-value above 1e-30 (arbitrary choice) and store the xml response in a pickled file, not to spam the NCBI service too much during writing this post :wink:. It's also very slow to run, so I'll just load the pickled file in the next step.

#### Parse the blast results

We can now load the pickled results & parse them to make them a bit more workable

```{python}
#! eval: false
import pickle
import xml.etree.ElementTree as ET


file = open('data/blasthits.pkl', 'rb')

# dump information to that file
xml = pickle.load(file)
file.close()
# print(xml)
root = ET.fromstring(xml)
identifiers = []
evalues = []
for alignment in root.iter('Hit'):
    identifier = alignment.find('Hit_id').text
    # Extract the coverage
    hsp = alignment.find('Hit_hsps/Hsp')
    coverage = int(hsp.find('Hsp_query-to').text) / int(hsp.find('Hsp_query-from').text)
    # Extract the percentage of identity
    identity = float(hsp.find('Hsp_identity').text) / float(hsp.find('Hsp_align-len').text)

    # Extract the e-value
    evalue = float(hsp.find('Hsp_evalue').text)
    identifiers.append(identifier)
    evalues.append(evalue)
print(identifiers)
```

We end up here with 139 hits. As a reminder, when gathering all structures of the same protein we identified over 300 structures. I'm expecting them to be part of the hitlist found here. I tried to find any indication on the PDB subset used by the NCBI, but couldn't identify whether they use the full RCSB PDB or a subset only. In conclusion: this isn't useable as such. So I optimistically tried the blast service on uniprot and got even less hits. Digging into the detail I noticed that the ebi is just running an ncbi blast behind the scenes as well. 

Well that's a bummer. I have two options here, either I expand the hitlist, by identifying all other structures encompassing the same protein as the hits I got here. But the hits aren't unique as well per biomolecule, not sure what this corresponds to in the end. Or I'd need to basically build my own blast database locally & do everything by hand. Let me roughly outline the steps here:

For each PDB structure & chain:

- extract the expressed sequence
- add it to a blast db file with an identifier corresponding to the pdb code + chain code
- run the blast locally

#### Fall back to the RCSB PDB search

The RCSB PDB seems to offer a protein sequence search as API now as well & this is the one I decided to use in the end: 

```{python}
#| eval: false
import json
import requests
query="""{
  "query": {
    "type": "terminal",
    "service": "sequence",
    "parameters": {
      "evalue_cutoff": 1e-30,
      "identity_cutoff": 0.1,
      "sequence_type": "protein",
      "value": "QPMEEEEVETFAFQAEIAQLMSLIINTFYSNKEIFLRELISNSSDALDKIRYESLTDPSKLDSGKELHINLIPNKQDRTLTIVDTGIGMTKADLINNLGTIAKSGTKAFMEALQAGADISMIGQFGVGFYSAYLVAEKVTVITKHNDDEQYAWESSAGGSFTVRTDTGEPMGRGTKVILHLKEDQTEYLEERRIKEIVKKHSQFIGYPITLFVEKERDKEVSDDEAE"
    }
  },
  "request_options": {
    "results_verbosity": "verbose",
    "scoring_strategy": "sequence",
    "return_all_hits": true
  },
  "return_type": "polymer_entity"
}"""


url="https://search.rcsb.org/rcsbsearch/v2/query?json="+query
response=requests.get(url)
hits=json.loads(response.text)["result_set"]
print(hits[0])
```

Here we get all structures (510 hits by the time I'm writing this) & even a bit more information which portion of the input sequence hit where on the structure. Be gentle with the calls to the RCSB, else they will blacklist your query. 

Now let's start meddling with the sequence mess and integrate already all the 
clustering bits we did in the previous post. 

### Clustering the similar binding sites

In the previous clustering I did on identical structures I didn't need to bother about residue numberings as they were identical in the end for all human HSP90 alpha structures. Here, however, I'll retrieve sequences from other organisms etc and it's guaranteed that the residue numberings won't coincide most of the time with the ones I used before. 

As a result the code I provide here includes a proper sequence mapping on the results from the RCSB query directly. 

```{python}
#| code-fold: true
#| output: false
#| eval: true

import gemmi
from gemmi import cif
import urllib
import numpy as np
from scipy.spatial.distance import cdist
import scipy.spatial.distance as ssd
import scipy
import matplotlib.pyplot as plt
import pandas as pd
from IPython.display import Markdown
from tabulate import tabulate
import blosum
subst_matrix = blosum.BLOSUM(62)

```
```{python}
#| code-fold: true
#| eval: false

#get all polymer identifiers from previous results
polymer_identifiers=[hit["identifier"] for hit in hits]

#now let's build another graphql query against the rcsb to get the author defined chain names of the structures that correspond to the polymer identified. 
query="""
{
  polymer_entities(entity_ids: """+str(polymer_identifiers).replace("'","\"")+""")
  {
    rcsb_id
    rcsb_polymer_entity_container_identifiers {
      auth_asym_ids
    }
    rcsb_polymer_entity_align{
      reference_database_name
            aligned_regions{
              entity_beg_seq_id
              length
              ref_beg_seq_id
            }
    }
  }
}
"""
url=f"https://data.rcsb.org/graphql?query={query}"
response=requests.get(url)
chains_tmp=response.json()["data"]["polymer_entities"]

#here we know what polymer id corresponds to what chain name
chain_polymer_mapping={chain["rcsb_id"]:chain["rcsb_polymer_entity_container_identifiers"]["auth_asym_ids"] for chain in chains_tmp}

#From our blast hits, let's keep only the sequence matching bits to simplify the object a bit:
clean_hits={hit["identifier"]:hit["services"][0]["nodes"][0]["match_context"][0] for hit in hits}

#Let's get our ATP binding site residue list mapping from our initial sequence: 
residue_list=np.array([47,48,51,52,54,55,58,93,95,96,97,98,106,107,112,131,132,133,134,135,136,137,138,139,152,183,184,186])-9


def transformResidueNumbers(residue_numbers, rcsbpolymerhit):
  df = pd.DataFrame(columns = ["aa_query", "aa_subject", "query_residue_number","subject_residue_number"])

  currentindex=0
  subject_aligned_seq_list=list(rcsbpolymerhit["subject_aligned_seq"])
  query_current_residue_number=rcsbpolymerhit["query_beg"]
  subject_current_residue_number=rcsbpolymerhit["subject_beg"]
  for idx,queryaa in enumerate(list(rcsbpolymerhit["query_aligned_seq"])):
    if(queryaa!='-'):
      query_residue_number=query_current_residue_number
      query_current_residue_number+=1
    else: 
      query_residue_number=-1
    if(subject_aligned_seq_list[idx]!='-'):
      subject_residue_number=subject_current_residue_number
      subject_current_residue_number+=1
    else:
      subject_residue_number=-1
    new_row=pd.DataFrame({"aa_query":queryaa,"aa_subject":subject_aligned_seq_list[idx],"query_residue_number":query_residue_number,"subject_residue_number":subject_residue_number}, index=[0])
    df=pd.concat([df.loc[:],new_row]).reset_index(drop=True)
  
  subject_residue_list=[df.loc[df['query_residue_number'] == residue_number, 'aa_subject'].values[0] for residue_number in residue_numbers]
  return((df.loc[df['query_residue_number'].isin(residue_numbers),"subject_residue_number"].tolist(),subject_residue_list))


def getContactMatrix(pdbCode, chainCode, residueSelection, debug=False):
  content= urllib.request.urlopen("https://files.rcsb.org/view/"+pdbCode+".cif").read()

  block=cif.read_string(content)[0]
  structure=gemmi.make_structure_from_block(block)
  positions=[]
  for model in structure:
    if model.name=="1":
      for chain in model:
        if chain.name == chainCode:
          for residue in chain:
            if residue.label_seq in residueSelection:
              if debug:
                print(residue.seqid)
                print(residue)
              for atom in residue:
                if atom.name=="CA":
                  if debug: print("ok")
                  positions.append(atom.pos.tolist())
                  break #we need that for multiple occurences

  if(len(positions)!=len(residueSelection)):
    print("Not all positions found for "+pdbCode+" discarding structure")
    return None

  positions_np=np.array(positions)
  return cdist(positions_np, positions_np, 'euclidean')


# create the contact matrices (CA based for now)
contactMatrices=[]
resultResidueNames=[]
selectedResidues=[]

# for pdbid in ['1UYF_1']:
for pdbid in list(chain_polymer_mapping.keys()):
  print(pdbid)
  chain=chain_polymer_mapping[pdbid][0] #take only the first chain
  pdbcode=pdbid.split("_")[0]
  tmp_result=transformResidueNumbers(residue_list,clean_hits[pdbid])
  mapped_residues=tmp_result[0]
  contactMatrices.append(getContactMatrix(pdbcode,chain,mapped_residues))
  resultResidueNames.append(tmp_result[1])
  selectedResidues.append([chain+":"+str(res) for res in mapped_residues])



none_indices = [ic for ic, matrix in enumerate(contactMatrices) if matrix is None]

labels=[structure.split("_")[0] for structure in list(chain_polymer_mapping.keys())]
chainCodes=[chain_polymer_mapping[structure][0] for structure in list(chain_polymer_mapping.keys())]
filteredContactMatrices = [matrix for i, matrix in enumerate(contactMatrices) if i not in none_indices]
filteredresultResidueNames = [bl for i, bl in enumerate(resultResidueNames) if i not in none_indices]
filteredSelectedResidues = [bl for i, bl in enumerate(selectedResidues) if i not in none_indices]



filteredLabels = [label for idx, label in enumerate(labels) if idx not in none_indices]
filteredChainCodes= [code for idx, code in enumerate(chainCodes) if idx not in none_indices]
finalList=pd.DataFrame(({"pdbid":filteredLabels,"chain":filteredChainCodes,"residues":filteredSelectedResidues}))
finalList.to_csv("data/finalList.tsv",sep="\t",index=False)
#final clustering of contact matrices


file = open('data/contactMatrices.pkl', 'wb')

# dump information to that file
pickle.dump({"contactMatrices":filteredContactMatrices,"fileredResidueNames":filteredresultResidueNames,"filteredLabels":filteredLabels,"filteredChainCodes":filteredChainCodes,"finalList":finalList}, file)

# close the file
file.close()


```





```{python}
#| output: false
#| eval: true
import numpy as np 

def clusterMatrices(matrixList,selectedResidueList,blosumWeight=1.0):  
  nMatrices=len(matrixList)
  result=np.zeros((nMatrices,nMatrices))
  for i in range(nMatrices):
    for j in range(nMatrices):
      if i==j:
        result[i][j]=0.0
      elif i<j:
        r1=selectedResidueList[i]
        r2=selectedResidueList[j]
        blosum_score=(np.array([subst_matrix[r1[idx]][r2[idx]] for idx in range(len(r1))])<0.0).sum()
        result[i][j]=np.mean(np.abs(matrixList[i]-matrixList[j]))+blosumWeight*blosum_score
        result[j][i]=result[i][j]

  distArray = ssd.squareform(result)
  clusters=scipy.cluster.hierarchy.linkage(result, method='single', metric='euclidean')
  return(clusters)



file = open('data/contactMatrices.pkl', 'rb')
contactMatrices = pickle.load(file)
file.close()

clusters=clusterMatrices(contactMatrices["contactMatrices"],contactMatrices["fileredResidueNames"])

```

```{python}
#| fig-cap: Result of hierarchical clustering of HSP90 ATP binding sites based on CA positions
#| label: fig-clustering-final
#| eval: true
#| code-fold: true

plt.figure(figsize=(9, 100))

scipy.cluster.hierarchy.dendrogram(clusters,labels=contactMatrices["filteredLabels"],orientation='right',leaf_font_size=9,color_threshold=2.0)
plt.show()
```

Well that was a bit painful to set up. However in the figure above now we should have a good hierarchical tree for a pure backbone geometry based comparison. 
The pocket comparison method we'd might want to evaluate later might make use of particular amino acids or side chain positions. Thus, I already integrated a bit of code to account for a penalty in the distance between two binding sites if unfavourable permutations were found (blosum62 score<0). Feel free to adapt to whatever you need. In the current code, an unfavourable substitution of an amino acid is equivalent to a 1A distance between the two alpha carbons. The hierarchical tree was colored at a cut distance of 2A. We can however observe 3 smaller clusters in the large pink cluster. This very likely corresponds to the 3 clusters we identified on structures of identical sequence. 

In the end we get a full set of structures, HSP90 alpha human ones, and similars. Let's check how much of our dataset established in the previous post. In theory we should be able to gather 100% of it through the approach outlined here as well. 

```{python}
#| code-fold: true
#| eval: true
import pickle

file = open('data/identicallist.pkl', 'rb')
identical_list = pickle.load(file)
file.close()

similar_list = [contactMatrices["filteredLabels"][idx]+":"+code for idx, code in enumerate(contactMatrices["filteredChainCodes"])]

intersection = np.intersect1d(identical_list, similar_list)

print(f"{len(intersection)} common structures vs {len(similar_list)} in the similar list and {len(identical_list)} in the identical list")


```

That's a nice and expected outcome here. All structures identified from our previous post are found through the approach used here as well. But instead of only 295 structures, we now have a total of 477 structures with different levels of similarity between all structures. 

## Results

I won't put all the raw results in a table format in HTML here, but [you can get the final list of structures, chain codes & residue selections as download from here](data/finalList.tsv). The tsv file is structured like this: 

```{python}
#| label: tbl-final
#| tbl-cap: First ten rows of the final list of structures, chain codes & residue selections
#| echo: false
#| eval: true



Markdown(tabulate(
  contactMatrices["finalList"].head(10),
  headers=["pdbid","chain", "residues"]
))

```

### How can this be used as benchmark dataset?

Let's come back to our initial problem. How can this dataset now be used to evaluate the performance of a pocket comparison method? 
The dataset contains information about:

- what binding site is similar to another
- to what extent the binding sites are similar (more or less similar)

So we already have a set of structures that we can in theory compare to one another and see if the binding site comparison similarity metric correlates with the similarity we considered here. 
We can also use the dataset to compare to structures outside of the set, a decoy set. This is in theory easy to set up but has important implications on how to evaluate a performance of a method. 

First let's clean up a bit and define a function that allows us to get the most similar binding sites from our set versus a chosen query structure:
  
```{python}
#| code-fold: true
#| label: tbl-similarity-sample
#| tbl-cap: Excerpt of the similarity matrix of binding sites
#| eval: true
import itertools

def getDistanceMatrix(matrixList,selectedResidueList,blosumWeight=1.0):  
  nMatrices=len(matrixList)
  result=np.zeros((nMatrices,nMatrices))
  for i in range(nMatrices):
    for j in range(nMatrices):
      if i==j:
        result[i][j]=0.0
      elif i<j:
        r1=selectedResidueList[i]
        r2=selectedResidueList[j]
        blosum_score=(np.array([subst_matrix[r1[idx]][r2[idx]] for idx in range(len(r1))])<0.0).sum()
        result[i][j]=np.mean(np.abs(matrixList[i]-matrixList[j]))+blosumWeight*blosum_score
        result[j][i]=result[i][j]

  return(result)

distanceMatrix=getDistanceMatrix(contactMatrices["contactMatrices"],contactMatrices["fileredResidueNames"])

np_similar_list=np.array(similar_list)
sortedMatrix=[]
for rowid in range(0,distanceMatrix.shape[0]):
  o=np.argsort(distanceMatrix[rowid,:])
  np_similar_list[o]
  sortedMatrix.append([np_similar_list[rowid],list(np.char.add(np.char.add(np_similar_list[o],": "),distanceMatrix[rowid,o].astype('<U7')))])


sortedMatrixDF=pd.DataFrame(sortedMatrix,columns=["pdbid","similar structures"])
sortedMatrixDF.to_csv("data/similarityList.tsv",sep="\t",index=False)

truncatedMatrix=[[element[0],element[1][:10]] for element in sortedMatrix[:10]]
Markdown(tabulate(
  truncatedMatrix[:10],
  headers=["pdbid","similar structures"]
))
```

Table @tbl-similarity-sample shows only the first 10 most similar structures and underlying binding sites to the binding site corresponding to the **pdbid** on the leftmost column. 

You can retrieve the full list through the downloadable tsv file here: [similarityList.tsv](data/similarityList.tsv)

This file can be used for various purposes. You can decide to reduce the dataset to reduce structural reduncany. You can also use it to train a method, parameters, scores on how similarity is defined etc.

#### Decoys?

In all benchmark datasets that I have seen so far in the litterature there's a distinction between binding sites one should find (actives) and binding sites one shouldn't find (decoys). 
I fully understand that this is rather standard practice for similarity metric performance measurements & evaluation studies. However, in the context of binding site comparison on a large variety of structures (like against the PDB) things get a little bit trickier. 


## Assessing ligand similarity in similar binding sites 

One interesting aspect about HSP90 is that there are a lot of structures in the RCSB. A lot of these structures have soaked or cocrystallized ligands from similar or congeneric chemical series. One interesting aspect to analyze is how similar two ligands are to each other when they are known to bind to very similar binding site conformations.

In order to try integrate that we first need to gather the ligands contained in the binding site encompassed by the residues selected, which aren't ions or surfactants or water. I didn't find a clear & easy way to get ligands contained within a given set of residues using RCSB resources, so I'll calculate the center of mass of the binding site residues here and check against the center of mass of all ligands in the structure. Very cumbersome but should do the trick here.

```{python}
#| eval: false
#| code-fold: true

from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit.Chem import rdFMCS
from rdkit import DataStructs
from rdkit.Chem import rdMolDescriptors
import seaborn as sns
import io


def getLigandHetCodeInBindingSite(pdbCode, chainCode, residueSelection, debug=False):
  content= urllib.request.urlopen("https://files.rcsb.org/view/"+pdbCode+".cif").read()
  block=cif.read_string(content)[0]
  structure=gemmi.make_structure_from_block(block)
  positions=[]
  hetDict={}
  for model in structure:
    if model.name=="1":
      for chain in model:
        if chain.name == chainCode:
          for residue in chain:
            if residue.label_seq in residueSelection:
              if debug:
                print(residue.seqid)
                print(residue)
              for atom in residue:
                if atom.name=="CA":
                  if debug: print("ok")
                  positions.append(atom.pos.tolist())
                  break #we need that for multiple occurences
            elif(str(residue.entity_type)=="EntityType.NonPolymer" and (str(residue.name) !="HOH" or str(residue.name)!="WAT")):
              dname=residue.name+":"+chain.name+":"+str(residue.seqid.num)
              hetDict[dname]=[]
              for atom in residue:
                  hetDict[dname].append(atom.pos.tolist())

  if(len(positions)!=len(residueSelection)):
    print("Not all positions found for "+pdbCode+" discarding structure")
    return None
  binding_site_com=np.mean(np.array(positions))
  results=[]
  if(len(hetDict.keys())>0):
    for ligand in hetDict.keys():
      ligand_com=np.mean(np.array(hetDict[ligand]))
      ligand_distance=np.linalg.norm(binding_site_com-ligand_com)
      results.append([ligand,ligand_distance])
  return results
  


hetatms=[]
for index,row in contactMatrices["finalList"].iterrows():
  hetatms.append(getLigandHetCodeInBindingSite(row["pdbid"],row["chain"],[int(el.split(":")[1]) for el in row["residues"]]))

retainedhetatmcodes=[]
threshold=3.0
for index,row in contactMatrices["finalList"].iterrows():
  tmp=[hetatm[0].split(":")[0] for hetatm in hetatms[index]if hetatm[1]<threshold]
  if(len(tmp)>0):
    
    retainedhetatmcodes+=[tmp]
  else:
    retainedhetatmcodes+=[['']]

contactMatrices["finalList"]["ligands"]=retainedhetatmcodes
flatretainedhetatmcodes = list(itertools.chain(*retainedhetatmcodes))


query="""
{
  chem_comps(comp_ids: """+str(flatretainedhetatmcodes).replace("'","\"")+""") {
    rcsb_id
    chem_comp {
      
      formula_weight
    }
    rcsb_chem_comp_descriptor{
      SMILES
    }
  }
}
"""

moleculedict={}
url=f"https://data.rcsb.org/graphql?query={query}"
response=requests.get(url)
dataRCSB=response.json()["data"]["chem_comps"]

for compound in dataRCSB:
  if compound["chem_comp"]["formula_weight"]>180.0:
    moleculedict[compound["rcsb_id"]]={"molecule":Chem.MolFromSmiles(compound["rcsb_chem_comp_descriptor"]["SMILES"])}

```

```{python}
#| eval: false
import base64
plots=[]
refmolecules=[]
hetcodes=[]
pdbcodes=[]
for index,row in contactMatrices["finalList"].iterrows():
  if index<10:
    print(index,retainedhetatmcodes[index])
    t=sortedMatrixDF.iloc[[index]]["similar structures"].values[0]
    sortedpdbcodes=[line.split(" ")[0].rstrip(":") for line in t]
    sortedpocketsimilarityscores=[float(line.split(" ")[1].rstrip(":")) for line in t]
    rowmolecules=[]
    fps=[]
    for pdbcode in sortedpdbcodes:
      tableindex=sortedMatrixDF.loc[sortedMatrixDF["pdbid"]==pdbcode].index[0]
      ligands=contactMatrices["finalList"].iloc[[tableindex]]["ligands"].values[0]
      for ligand in ligands:
        if ligand in moleculedict:
          rowmolecules.append(moleculedict[ligand]["molecule"])
          fps.append(rdMolDescriptors.GetMorganFingerprint(moleculedict[ligand]["molecule"],radius=2))
    reffp=fps[0]
    fpscores = DataStructs.BulkTanimotoSimilarity(reffp, fps)
    plotdata=pd.DataFrame(zip(sortedpocketsimilarityscores,fpscores),columns=["pocket dissimilarity","fp similarity"])
    g = sns.pairplot(plotdata)
    buf = io.BytesIO()
    g.fig.savefig(buf, format='png')
    buf.seek(0)
    image = base64.b64encode(buf.read())
    plots.append(image)
    refmolecules.append(rowmolecules[0])
    pdbcodes.append(row["pdbid"])
    hetcodes.append(retainedhetatmcodes[index])


# Draw.MolsToGridImage(rowmolecules[:20],molsPerRow=5,subImgSize=(300,200),legends=["Tanimoto: "+str(round(score,2)) for score in scores[:20]])

    # mcs = rdFMCS.FindMCS(rowmolecules,threshold=0.8,completeRingsOnly=True,ringMatchesRingOnly=True)
    # patt = Chem.MolFromSmarts(mcs.smartsString)
    # refMol = rowmolecules[0]
    # AllChem.Compute2DCoords(refMol)
    # refMatch = refMol.GetSubstructMatch(patt)
    # for probeMol in rowmolecules[1:]:
    #   AllChem.GenerateDepictionMatching2DStructure(query , refMatch)
```



```{python}
#| code-fold: true
#| eval: false

tableplotdata=pd.DataFrame(zip(pdbcodes,refmolecules,plots,hetcodes),columns=["pdb","molecule","similarity plot","ligand name"])

with open("data/similarityplots.pkl","wb") as f:
  pickle.dump(tableplotdata,f)


# Markdown(tabulate(
#   tableplotdata[:10],
#   headers=["pdbid","molecule","similarity"]
# ))


```


```{python}
#| code-fold: true
#| tbl-cap: Pocket Dissimilarity (0 similar, higher dissimilar) vs ligand Tanimoto similarity (higher similar, lower dissimilar)

from rdkit import Chem
from rdkit.Chem import PandasTools
import urllib

with open("data/similarityplots.pkl","rb") as f:
  tableplotdata=pickle.load(f)
  tableplotdata["smiles"]=[Chem.MolToSmiles(mol) for mol in tableplotdata["molecule"]]
  PandasTools.AddMoleculeColumnToFrame(tableplotdata,'smiles','molecule')
  for index, row in tableplotdata.iterrows():
    uri = 'data:image/png;base64,' + urllib.parse.quote(row["similarity plot"])
    html = '<img width="300px" src = "%s"/>' % uri
    row["similarity plot"]=html

tableplotdata.to_html(escape=False)

```

The table above summarizes for 10 out of over 400 structures the similarity of the binding sites versus the Tanimoto similarity of the ligands located in the binding sites. I used a Morgan fingerprint (radius) 2 and according to a [blogpost by Greg Landrum](https://greglandrum.github.io/rdkit-blog/posts/2021-05-18-fingerprint-thresholds1.html), the random threshold is situated at around 0.2 - 0.3. 

Feel free to adapt the code to display the plots for all structures.

When looking through the first few plots we can see that even though sometimes there might be some correlation between the two similarity metrics, for the large bulk there doesn't seem to be any correlation. Both similarity metrics can be discussed, refined, rendered more fuzzy or precise, one can also replace the ligand similarity by an MCS approach. Again, you can adapt the code quite easily to get there, but still the current results give a lot of examples where rather different ligands can bind to very similar binding sites. Also in several cases ligands above the significance threshold can be found in very similar pockets (compound series), but the opposite is also true.

Let's discuss in more detail a typical compound series found in the dataset. Structure 2h55 resulted in the following distribution: 

![Distribution of ligand similarity vs pocket similarity values when using 2h55 ligand DZ8 as reference](images/2h55_plot.png){#fig-2h55}

In the orange box on @fig-2h55 several ligands with some similarity to DZ8 are identified in very similar binding sites (pocket dissimilarity close to 0). More dissimilar binding sites do not seem to bind this class of ligands. This points into the direction that this series of compounds binds clearly to a particular conformation (or induces it, or both). 
It is also interesting to note that the majority of other compounds in the set are significantly dissimilar to the query ligand.

![Distribution of ligand similarity vs pocket similarity values when using 6b9a ligand PA7 as reference](images/6b9a_plot.png){#fig-6b9a}

On figure @fig-6b9a we can see a very different distribution. Here we have an ATP analog as query ligand & the corresponding binding site. 
It is very interesting to observe the skewness to more dissimilar binding sites compared to 6b9a. Despite the dissimilarity on binding sites we can see a large cluster of ligands with around 0.5 Tanimoto similarity to PA7. This cluster encompasses ATP & all the closer analogs. 
This example is a very good one to illustrate the complexity behind the theory of similar ligands bind to similar binding sites. ATP analogs clearly accomodate for a larger variety of binding site conformations & mutations than the compound series around DZ8 does. 
We could then even imagine that DZ8 is conformation selective & has a few hallmarks of a more specific inhibitor. That's a bit far fetched for a comparison with a plain ATP analog here, but it appears to go into that direction nevertheless. 
<!-- 

## Testing algorithms for rank order

Even though we don't have a full benchmark data-set to scrutinize every little detail of every algorithm, we can already run a few tests with what we have. 

We can for instance envision the following scenario: For structures in the multiple larger subclusters we observed on @fig-clustering-final, what if one of them was taken as query versus the full RCSB PDB. Would we find the closest analogs with a pocket comparison algorithm?

Now let's go into the tedious task of running some of the comparison algorithms against that data-set, considering the full PDB as background data (decoys). I'll start with 2 commercial solutions, for the main & simple reason that I don't have to prepare a dataset myself, but "just call an API". 

One important aspect of variability I'll encounter here is the scope & definition of the binding site in these tools. 3decision runs fpocket [@LeGuilloux2009], PSILO SiteFinder for instance. KRIPO [Ritschel2014] on the other hand uses only ligands to define a binding site (so bye bye apo structures, I guess). For all of these approaches I'll try to see how to identify the relevant bindning site of interest if used as a query.

### Algorithms I'll test (for now)

- **Discngine 3decision**: Since my last year of my PhD till now I've been trying to develop my own algorithms to run pocket comparison. Throughout the years the team grew & a few colleagues of mine joined the effort. As a result, Discngine 3decision integrates now our current integration of this work. It's accessible via the UI, but also through a REST API, which I'll use here instead. 

- **CCG PSILO**: PSILO is CCG's protein structure repository and Howard Feldman & Paul Labute have also integrated an algorithm in PSILO itself [@Feldman2010].

- **KRIPO**: Is on the list of the algorithms evaluated in the paper by @Ehrt2018. It's probably the only one I can use without too much development required to get me going on the level of the RCSB PDB. NB: it's an interaction based algorithm. 


### Discngine 3decision

Below you can find code to run a pocket comparison versus 3decision with a sample input structure & residue selection.  -->

